{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5663d8-7b91-49ae-9b15-ff2d05e9a1b8",
   "metadata": {},
   "source": [
    "# Rainforest Connection Species Audio Detection\n",
    "\n",
    "A Machine Learning project based on a [Kaggle Competition](https://www.kaggle.com/c/rfcx-species-audio-detection)\n",
    "\n",
    "Team Members: \n",
    "- Paloma Cartwright (palomacartwright@bren.ucsb.edu)\n",
    "- Halina Do-Linh (halina@bren.ucsb.edu)\n",
    "- Mia Forsline (mforsline@bren.ucsb.edu)\n",
    "- Dan Kerstan (danielkerstan@bren.ucsb.edu)\n",
    "- Scout Leonard (scout@bren.ucsb.edu)\n",
    "- Desik Somasundaram (desik@bren.ucsb.edu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5d155e-805b-4d69-b519-e93e80a96146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00c72c4-6daa-47b5-9bfc-587ab3ab4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/courses/EDS232/rainforest\"\n",
    "fft = 2048\n",
    "hop = 512\n",
    "sr = 48000\n",
    "length = 10 * sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b276e9a-3e97-4e1a-b566-6cf264749103",
   "metadata": {},
   "source": [
    "## Read in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4751ac5-228d-490b-9429-ac9430298017",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/courses/EDS232/rainforest/train_tp.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444d77c-5bbd-4191-940a-82035f7223ac",
   "metadata": {},
   "source": [
    "### Set minimum and maximum frequency values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cd9cd0-46d0-4b15-9f4b-04fe1bbb5196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 84, maximum frequency: 15056\n"
     ]
    }
   ],
   "source": [
    "# Check minimum/maximum frequencies for species calls\n",
    "# Not neccesary, but there are usually plenty of noise in low frequencies, and removing it helps\n",
    "fmin = 24000\n",
    "fmax = 0\n",
    "\n",
    "# Skip header row (recording_id, species_id, songtype_id, t_min, f_min, t_max, f_max) \n",
    "# and start from 1 instead of 0\n",
    "for i in range(1, len(data)):\n",
    "    if fmin > float(data[i][4]):\n",
    "        fmin = float(data[i][4])\n",
    "    if fmax < float(data[i][6]):\n",
    "        fmax = float(data[i][6])\n",
    "\n",
    "# Get some safety margin\n",
    "fmin = int(fmin * 0.9)\n",
    "\n",
    "fmax = int(fmax * 1.1)\n",
    "print('Minimum frequency: ' + str(fmin) + ', maximum frequency: ' + str(fmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30827f9b-36d1-4075-b6df-8ad883f28a95",
   "metadata": {},
   "source": [
    "## Convert audio files to Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfb12b0-9ebd-480c-adb4-4157434ae6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spectrogram generation\n",
      "Processed 100 train examples from 1217\n",
      "Processed 200 train examples from 1217\n",
      "Processed 300 train examples from 1217\n",
      "Processed 400 train examples from 1217\n",
      "Processed 500 train examples from 1217\n",
      "Processed 600 train examples from 1217\n",
      "Processed 700 train examples from 1217\n",
      "Processed 800 train examples from 1217\n",
      "Processed 900 train examples from 1217\n",
      "Processed 1000 train examples from 1217\n",
      "Processed 1100 train examples from 1217\n",
      "Processed 1200 train examples from 1217\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Starting spectrogram generation')\n",
    "for i in range(1, len(data)):\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/train/' + data[i][0] + '.flac', sr=None)\n",
    "    \n",
    "    t_min = float(data[i][3]) * sr\n",
    "    t_max = float(data[i][5]) * sr\n",
    "    \n",
    "    # Positioning sound slice\n",
    "    center = np.round((t_min + t_max) / 2)\n",
    "    beginning = center - length / 2\n",
    "    if beginning < 0:\n",
    "        beginning = 0\n",
    "    \n",
    "    ending = beginning + length\n",
    "    if ending > len(wav):\n",
    "        ending = len(wav)\n",
    "        beginning = ending - length\n",
    "        \n",
    "    slice = wav[int(beginning):int(ending)]\n",
    "    \n",
    "    # Mel spectrogram generation\n",
    "    # Default settings were bad, parameters are adjusted to generate somewhat \n",
    "    # reasonable quality images\n",
    "    # The better your images are, the better your neural net would perform\n",
    "    # You can also use librosa.stft + librosa.amplitude_to_db instead\n",
    "    mel_spec = librosa.feature.melspectrogram(slice, \n",
    "                                              n_fft = fft, \n",
    "                                              hop_length = hop, \n",
    "                                              sr = sr, \n",
    "                                              fmin = fmin, \n",
    "                                              fmax = fmax, \n",
    "                                              power = 1.5)\n",
    "    mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "    # Normalize to 0...1 - this is what goes into neural net\n",
    "    mel_spec = mel_spec - np.min(mel_spec)\n",
    "    mel_spec = mel_spec / np.max(mel_spec)\n",
    "\n",
    "    # And this 0...255 is for the saving in bmp format\n",
    "    mel_spec = mel_spec * 255\n",
    "    mel_spec = np.round(mel_spec)    \n",
    "    mel_spec = mel_spec.astype('uint8')\n",
    "    mel_spec = np.asarray(mel_spec)\n",
    "    \n",
    "    bmp = Image.fromarray(mel_spec, 'L')\n",
    "    bmp.save('/courses/EDS232/rainforest/working/' + data[i][0] + '_' + \n",
    "             data[i][1] + '_' + str(center) + '.bmp')\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Processed ' + str(i) + ' train examples from ' + str(len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0067567c-7a89-40d2-b6c2-0fcf47728c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_species = 24\n",
    "# 6GB GPU-friendly (~4 GB used by model)\n",
    "# Increase if neccesary\n",
    "batch_size = 16\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d7c9a-3d1a-4233-bbe1-28f691af63d3",
   "metadata": {},
   "source": [
    "## Creating the RainforestDataset Class for the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a552bb4c-ca2d-4c1c-8f7c-4cae15e1aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torchdata\n",
    "\n",
    "class RainforestDataset(torchdata.Dataset):\n",
    "    def __init__(self, filelist):\n",
    "        self.specs = []\n",
    "        self.labels = []\n",
    "        for f in filelist:\n",
    "            # Easier to pass species in filename at the start; \n",
    "            # worth changing later to more capable method\n",
    "            label = int(str.split(f, '_')[1])\n",
    "            label_array = np.zeros(num_species, dtype=np.single)\n",
    "            label_array[label] = 1.\n",
    "            self.labels.append(label_array)\n",
    "            \n",
    "            # Open and save spectrogram to memory\n",
    "            \n",
    "            # If you use more spectrograms (add train_fp, for example), \n",
    "            # then they would not all fit to memory\n",
    "            # In this case you should load them on the fly in __getitem__\n",
    "            img = Image.open('/courses/EDS232/rainforest/working/' + f)\n",
    "            mel_spec = np.array(img)\n",
    "            img.close()\n",
    "            \n",
    "            # Transforming spectrogram from bmp to 0..1 array\n",
    "            mel_spec = mel_spec / 255\n",
    "            # Stacking for 3-channel image for resnet\n",
    "            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "            \n",
    "            self.specs.append(mel_spec)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.specs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # Augment here if you want\n",
    "        return self.specs[item], self.labels[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda832b5-bf63-49dd-9932-849235b72d89",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb8d92e-f7bb-41f1-a66c-8419d59c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 972 examples\n",
      "Validating on 244 examples\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "label_list = []\n",
    "\n",
    "for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "    if '.bmp' in f:\n",
    "        file_list.append(f)\n",
    "        label = str.split(f, '_')[1]\n",
    "        label_list.append(label)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n",
    "    # Picking only first fold to train/val on\n",
    "    # This means loss of 20% training data\n",
    "    # To avoid this, you can train 5 different models on 5 folds and average predictions\n",
    "    if fold_id == 0:\n",
    "        train_files = np.take(file_list, train_index)\n",
    "        val_files = np.take(file_list, val_index)\n",
    "\n",
    "print('Training on ' + str(len(train_files)) + ' examples')\n",
    "print('Validating on ' + str(len(val_files)) + ' examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aac64c-1f5d-400d-b891-1677a32eeffa",
   "metadata": {},
   "source": [
    "## Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d412c7ef-0f68-43a8-8eab-b747a6320091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from resnest.torch import resnest50\n",
    "import torch.nn.functional as F\n",
    "# !cd /courses/EDS232/rainforest && curl -O https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "\n",
    "train_dataset = RainforestDataset(train_files)\n",
    "val_dataset = RainforestDataset(val_files)\n",
    "\n",
    "train_loader = torchdata.DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=torchdata.RandomSampler(train_dataset))\n",
    "\n",
    "val_loader = torchdata.DataLoader(val_dataset, \n",
    "                                  batch_size=batch_size, \n",
    "                                  sampler=torchdata.RandomSampler(val_dataset))\n",
    "\n",
    "# ResNeSt: Split-Attention Networks\n",
    "# https://arxiv.org/abs/2004.08955\n",
    "# Significantly outperforms standard Resnet\n",
    "\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "# Picked for this notebook; pick new ones after major changes \n",
    "# (such as adding train_fp to train data)\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.01, \n",
    "                            weight_decay=0.0001, \n",
    "                            momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                            step_size=7, \n",
    "                                            gamma=0.4)\n",
    "\n",
    "# This loss function is not exactly suited for competition metric, \n",
    "# which only cares about ranking of predictions\n",
    "# Exploring different loss fuctions would be a good idea\n",
    "pos_weights = torch.ones(num_species)\n",
    "pos_weights = pos_weights * num_species\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f912c9b-9b02-4db3-b6eb-aba415a180c5",
   "metadata": {},
   "source": [
    "## Training model on Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a391f1b8-3a7e-4ba3-9f0d-d2a117375bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "Epoch 0 training end. LR: 0.01, Loss: 1.361987096364381, Correct answers: 62/972\n",
      "Epoch 0 validation end. LR: 0.01, Loss: 1.3523476421833038, Correct answers: 20/244\n",
      "Saving new best model at epoch 0 (20/244)\n",
      "Epoch 1 training end. LR: 0.01, Loss: 1.3375267415750223, Correct answers: 76/972\n",
      "Epoch 1 validation end. LR: 0.01, Loss: 1.3528365567326546, Correct answers: 21/244\n",
      "Saving new best model at epoch 1 (21/244)\n",
      "Epoch 2 training end. LR: 0.01, Loss: 1.2585073455435332, Correct answers: 78/972\n",
      "Epoch 2 validation end. LR: 0.01, Loss: 1.2468452528119087, Correct answers: 26/244\n",
      "Saving new best model at epoch 2 (26/244)\n",
      "Epoch 3 training end. LR: 0.01, Loss: 1.197377107182487, Correct answers: 83/972\n",
      "Epoch 3 validation end. LR: 0.01, Loss: 1.199800118803978, Correct answers: 24/244\n",
      "Epoch 4 training end. LR: 0.01, Loss: 1.1609966227265656, Correct answers: 95/972\n",
      "Epoch 4 validation end. LR: 0.01, Loss: 1.1652445197105408, Correct answers: 24/244\n",
      "Epoch 5 training end. LR: 0.01, Loss: 1.140672640722306, Correct answers: 103/972\n",
      "Epoch 5 validation end. LR: 0.01, Loss: 1.2409926056861877, Correct answers: 14/244\n",
      "Epoch 6 training end. LR: 0.01, Loss: 1.1232368692022856, Correct answers: 97/972\n",
      "Epoch 6 validation end. LR: 0.01, Loss: 1.2036396488547325, Correct answers: 21/244\n",
      "Epoch 7 training end. LR: 0.004, Loss: 1.0868714070710979, Correct answers: 112/972\n",
      "Epoch 7 validation end. LR: 0.004, Loss: 1.1137681119143963, Correct answers: 37/244\n",
      "Saving new best model at epoch 7 (37/244)\n",
      "Epoch 8 training end. LR: 0.004, Loss: 1.055814130384414, Correct answers: 127/972\n",
      "Epoch 8 validation end. LR: 0.004, Loss: 1.0652800723910332, Correct answers: 31/244\n",
      "Epoch 9 training end. LR: 0.004, Loss: 1.0178350386072377, Correct answers: 131/972\n",
      "Epoch 9 validation end. LR: 0.004, Loss: 1.0476772598922253, Correct answers: 35/244\n",
      "Epoch 10 training end. LR: 0.004, Loss: 1.0110464467376958, Correct answers: 151/972\n",
      "Epoch 10 validation end. LR: 0.004, Loss: 1.114810012280941, Correct answers: 30/244\n",
      "Epoch 11 training end. LR: 0.004, Loss: 1.0091377027699204, Correct answers: 155/972\n",
      "Epoch 11 validation end. LR: 0.004, Loss: 1.0515890680253506, Correct answers: 30/244\n",
      "Epoch 12 training end. LR: 0.004, Loss: 1.0073457252783853, Correct answers: 177/972\n",
      "Epoch 12 validation end. LR: 0.004, Loss: 1.0701630264520645, Correct answers: 34/244\n",
      "Epoch 13 training end. LR: 0.004, Loss: 0.953763370631171, Correct answers: 208/972\n",
      "Epoch 13 validation end. LR: 0.004, Loss: 1.0178424157202244, Correct answers: 44/244\n",
      "Saving new best model at epoch 13 (44/244)\n",
      "Epoch 14 training end. LR: 0.0016, Loss: 0.9169219224179377, Correct answers: 202/972\n",
      "Epoch 14 validation end. LR: 0.0016, Loss: 0.9964558444917202, Correct answers: 41/244\n",
      "Epoch 15 training end. LR: 0.0016, Loss: 0.8652958312972647, Correct answers: 233/972\n",
      "Epoch 15 validation end. LR: 0.0016, Loss: 1.0136466287076473, Correct answers: 49/244\n",
      "Saving new best model at epoch 15 (49/244)\n",
      "Epoch 16 training end. LR: 0.0016, Loss: 0.8595739378303778, Correct answers: 223/972\n",
      "Epoch 16 validation end. LR: 0.0016, Loss: 0.9884754568338394, Correct answers: 59/244\n",
      "Saving new best model at epoch 16 (59/244)\n",
      "Epoch 17 training end. LR: 0.0016, Loss: 0.8624002190886951, Correct answers: 232/972\n",
      "Epoch 17 validation end. LR: 0.0016, Loss: 1.048056986182928, Correct answers: 53/244\n",
      "Epoch 18 training end. LR: 0.0016, Loss: 0.8339749281523657, Correct answers: 255/972\n",
      "Epoch 18 validation end. LR: 0.0016, Loss: 0.995983611792326, Correct answers: 51/244\n",
      "Epoch 19 training end. LR: 0.0016, Loss: 0.8013212944640488, Correct answers: 269/972\n",
      "Epoch 19 validation end. LR: 0.0016, Loss: 1.031503014266491, Correct answers: 59/244\n",
      "Epoch 20 training end. LR: 0.0016, Loss: 0.7985584442732764, Correct answers: 255/972\n",
      "Epoch 20 validation end. LR: 0.0016, Loss: 1.1278629191219807, Correct answers: 48/244\n",
      "Epoch 21 training end. LR: 0.00064, Loss: 0.7546938550276835, Correct answers: 287/972\n",
      "Epoch 21 validation end. LR: 0.00064, Loss: 1.0544383600354195, Correct answers: 59/244\n",
      "Epoch 22 training end. LR: 0.00064, Loss: 0.7412478914026355, Correct answers: 289/972\n",
      "Epoch 22 validation end. LR: 0.00064, Loss: 0.9670629389584064, Correct answers: 63/244\n",
      "Saving new best model at epoch 22 (63/244)\n",
      "Epoch 23 training end. LR: 0.00064, Loss: 0.7096698479574235, Correct answers: 317/972\n",
      "Epoch 23 validation end. LR: 0.00064, Loss: 1.048965621739626, Correct answers: 61/244\n",
      "Epoch 24 training end. LR: 0.00064, Loss: 0.6807401610202477, Correct answers: 324/972\n",
      "Epoch 24 validation end. LR: 0.00064, Loss: 1.0682231336832047, Correct answers: 56/244\n",
      "Epoch 25 training end. LR: 0.00064, Loss: 0.6920348454694278, Correct answers: 328/972\n",
      "Epoch 25 validation end. LR: 0.00064, Loss: 1.077917717397213, Correct answers: 61/244\n",
      "Epoch 26 training end. LR: 0.00064, Loss: 0.6708779188453174, Correct answers: 312/972\n",
      "Epoch 26 validation end. LR: 0.00064, Loss: 1.055585939437151, Correct answers: 57/244\n",
      "Epoch 27 training end. LR: 0.00064, Loss: 0.6705774625793832, Correct answers: 341/972\n",
      "Epoch 27 validation end. LR: 0.00064, Loss: 1.0673721097409725, Correct answers: 70/244\n",
      "Saving new best model at epoch 27 (70/244)\n",
      "Epoch 28 training end. LR: 0.00025600000000000004, Loss: 0.6378333353605427, Correct answers: 351/972\n",
      "Epoch 28 validation end. LR: 0.00025600000000000004, Loss: 1.0184819251298904, Correct answers: 67/244\n",
      "Epoch 29 training end. LR: 0.00025600000000000004, Loss: 0.6126193169687615, Correct answers: 384/972\n",
      "Epoch 29 validation end. LR: 0.00025600000000000004, Loss: 1.0264469496905804, Correct answers: 67/244\n",
      "Epoch 30 training end. LR: 0.00025600000000000004, Loss: 0.6256510341753725, Correct answers: 383/972\n",
      "Epoch 30 validation end. LR: 0.00025600000000000004, Loss: 1.0122696980834007, Correct answers: 70/244\n",
      "Epoch 31 training end. LR: 0.00025600000000000004, Loss: 0.6273593228371417, Correct answers: 370/972\n",
      "Epoch 31 validation end. LR: 0.00025600000000000004, Loss: 1.01280864700675, Correct answers: 65/244\n",
      "Epoch 32 training end. LR: 0.00025600000000000004, Loss: 0.618442209529095, Correct answers: 380/972\n",
      "Epoch 32 validation end. LR: 0.00025600000000000004, Loss: 1.0355839803814888, Correct answers: 67/244\n",
      "Epoch 33 training end. LR: 0.00025600000000000004, Loss: 0.6244517642943586, Correct answers: 367/972\n",
      "Epoch 33 validation end. LR: 0.00025600000000000004, Loss: 1.0223441123962402, Correct answers: 68/244\n",
      "Epoch 34 training end. LR: 0.00025600000000000004, Loss: 0.6127948057456095, Correct answers: 360/972\n",
      "Epoch 34 validation end. LR: 0.00025600000000000004, Loss: 1.0605731792747974, Correct answers: 72/244\n",
      "Saving new best model at epoch 34 (72/244)\n",
      "Epoch 35 training end. LR: 0.00010240000000000002, Loss: 0.5975835152336808, Correct answers: 392/972\n",
      "Epoch 35 validation end. LR: 0.00010240000000000002, Loss: 1.0229822881519794, Correct answers: 71/244\n",
      "Epoch 36 training end. LR: 0.00010240000000000002, Loss: 0.5963208699812654, Correct answers: 377/972\n",
      "Epoch 36 validation end. LR: 0.00010240000000000002, Loss: 1.0416265539824963, Correct answers: 68/244\n",
      "Epoch 37 training end. LR: 0.00010240000000000002, Loss: 0.5821126880215817, Correct answers: 414/972\n",
      "Epoch 37 validation end. LR: 0.00010240000000000002, Loss: 1.0127435624599457, Correct answers: 71/244\n",
      "Epoch 38 training end. LR: 0.00010240000000000002, Loss: 0.6050851418346656, Correct answers: 384/972\n",
      "Epoch 38 validation end. LR: 0.00010240000000000002, Loss: 1.0409399271011353, Correct answers: 73/244\n",
      "Saving new best model at epoch 38 (73/244)\n",
      "Epoch 39 training end. LR: 0.00010240000000000002, Loss: 0.6079637926132953, Correct answers: 366/972\n",
      "Epoch 39 validation end. LR: 0.00010240000000000002, Loss: 1.0181066170334816, Correct answers: 69/244\n"
     ]
    }
   ],
   "source": [
    "best_corrects = 0\n",
    "\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 40):\n",
    "    # Stats\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        vals, answers = torch.max(output, 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + \n",
    "          str(train_dataset.__len__()))\n",
    "    \n",
    "    # Single epoch - validation\n",
    "    with torch.no_grad():\n",
    "        # Stats\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        \n",
    "        model.eval()\n",
    "        for batch, (data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            # Stats\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "        \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, \n",
    "    # but in this case our loss is misaligned with competition metric, \n",
    "    # making accuracy a better metric\n",
    "    if sum(val_corr) > best_corrects:\n",
    "        print('Saving new best model at epoch ' + str(e) + ' (' + \n",
    "              str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "        torch.save(model, '/courses/EDS232/rainforest/best_model.pt')\n",
    "        best_corrects = sum(val_corr)\n",
    "        \n",
    "    # Call every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be892a30-47e0-4aee-b8af-5067a40a39df",
   "metadata": {},
   "source": [
    "### Function to split and load one test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1086c821-9621-4f97-b52c-f6a67dacff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(f):\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/test/' + f, sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, \n",
    "                                                  n_fft=fft, \n",
    "                                                  hop_length=hop, \n",
    "                                                  sr=sr, \n",
    "                                                  fmin=fmin, \n",
    "                                                  fmax=fmax, \n",
    "                                                  power=1.5)\n",
    "        mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "          \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f46ca4-9f52-4ccf-89ec-ff72287989e6",
   "metadata": {},
   "source": [
    "## Predict Species for each recording using Best Model\n",
    "\n",
    "The best model was stored in `best_model.pt` output file from the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9258e2c9-8150-4fb5-8d06-17cebe07df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop\n",
      "1992\n",
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "Submission generated\n"
     ]
    }
   ],
   "source": [
    "save_to_disk = 0\n",
    "\n",
    "# Loading model back\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "model = torch.load('/courses/EDS232/rainforest/best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "# Scoring does not like many files:(\n",
    "if save_to_disk == 0:\n",
    "    for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "        os.remove('/courses/EDS232/rainforest/working/' + f)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open('/courses/EDS232/rainforest/submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10',\n",
    "                                's11', 's12','s13','s14','s15','s16','s17','s18','s19','s20','s21',\n",
    "                                's22','s23'])\n",
    "    \n",
    "    test_files = os.listdir('/courses/EDS232/rainforest/test/')\n",
    "    print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = load_test_file(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, \n",
    "        # and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9d775-a469-4665-8e21-b63ff68bbd55",
   "metadata": {},
   "source": [
    "## Perform Logit Transformation on Submission Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a0b6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(base_dir, \"submission.csv\"))\n",
    "\n",
    "#make the recording id the index\n",
    "submission = submission.set_index(\"recording_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1268cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e18e1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission generated, gorls!\n"
     ]
    }
   ],
   "source": [
    "# create a new csv file with sigmoid values\n",
    "\n",
    "with open(\"/courses/EDS232/rainforest/submission_logit.csv\", 'w', newline = '') as csvfile2:\n",
    "    \n",
    "    logit_sub = submission.applymap(sigmoid)\n",
    "    logit_sub.to_csv(csvfile2, header = ['s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10',\n",
    "                                's11', 's12','s13','s14','s15','s16','s17','s18','s19','s20','s21',\n",
    "                                's22','s23'])\n",
    "\n",
    "print(\"submission generated, gorls!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
