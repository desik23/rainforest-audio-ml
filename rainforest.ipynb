{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5d155e-805b-4d69-b519-e93e80a96146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00c72c4-6daa-47b5-9bfc-587ab3ab4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/courses/EDS232/rainforest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4751ac5-228d-490b-9429-ac9430298017",
   "metadata": {},
   "outputs": [],
   "source": [
    "fft = 2048\n",
    "hop = 512\n",
    "# Less rounding errors this way\n",
    "sr = 48000\n",
    "length = 10 * sr\n",
    "\n",
    "with open('/courses/EDS232/rainforest/train_tp.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)\n",
    "\n",
    "# data = pd.read_csv(os.path.join(base_dir, 'train_tp.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cd9cd0-46d0-4b15-9f4b-04fe1bbb5196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 84, maximum frequency: 15056\n"
     ]
    }
   ],
   "source": [
    "# Check minimum/maximum frequencies for bird calls\n",
    "# Not neccesary, but there are usually plenty of noise in low frequencies, and removing it helps\n",
    "fmin = 24000\n",
    "fmax = 0\n",
    "\n",
    "\n",
    "# Skip header row (recording_id,species_id,songtype_id,t_min,f_min,t_max,f_max) and start from 1 instead of 0\n",
    "for i in range(1, len(data)):\n",
    "    if fmin > float(data[i][4]):\n",
    "        fmin = float(data[i][4])\n",
    "    if fmax < float(data[i][6]):\n",
    "        fmax = float(data[i][6])\n",
    "\n",
    "# Get some safety margin\n",
    "fmin = int(fmin * 0.9)\n",
    "\n",
    "fmax = int(fmax * 1.1)\n",
    "print('Minimum frequency: ' + str(fmin) + ', maximum frequency: ' + str(fmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfb12b0-9ebd-480c-adb4-4157434ae6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spectrogram generation\n",
      "Processed 100 train examples from 1217\n",
      "Processed 200 train examples from 1217\n",
      "Processed 300 train examples from 1217\n",
      "Processed 400 train examples from 1217\n",
      "Processed 500 train examples from 1217\n",
      "Processed 600 train examples from 1217\n",
      "Processed 700 train examples from 1217\n",
      "Processed 800 train examples from 1217\n",
      "Processed 900 train examples from 1217\n",
      "Processed 1000 train examples from 1217\n",
      "Processed 1100 train examples from 1217\n",
      "Processed 1200 train examples from 1217\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Starting spectrogram generation')\n",
    "for i in range(1, len(data)):\n",
    "    # All sound files are 48000 bitrate, no need to slowly resample\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/train/' + data[i][0] + '.flac', sr=None)\n",
    "    \n",
    "    t_min = float(data[i][3]) * sr\n",
    "    t_max = float(data[i][5]) * sr\n",
    "    \n",
    "    # Positioning sound slice\n",
    "    center = np.round((t_min + t_max) / 2)\n",
    "    beginning = center - length / 2\n",
    "    if beginning < 0:\n",
    "        beginning = 0\n",
    "    \n",
    "    ending = beginning + length\n",
    "    if ending > len(wav):\n",
    "        ending = len(wav)\n",
    "        beginning = ending - length\n",
    "        \n",
    "    slice = wav[int(beginning):int(ending)]\n",
    "    \n",
    "    # Mel spectrogram generation\n",
    "    # Default settings were bad, parameters are adjusted to generate somewhat \n",
    "    # reasonable quality images\n",
    "    # The better your images are, the better your neural net would perform\n",
    "    # You can also use librosa.stft + librosa.amplitude_to_db instead\n",
    "    mel_spec = librosa.feature.melspectrogram(slice, n_fft=fft, hop_length=hop, sr=sr, fmin=fmin, fmax=fmax, power=1.5)\n",
    "    mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "    # Normalize to 0...1 - this is what goes into neural net\n",
    "    mel_spec = mel_spec - np.min(mel_spec)\n",
    "    mel_spec = mel_spec / np.max(mel_spec)\n",
    "\n",
    "    # And this 0...255 is for the saving in bmp format\n",
    "    mel_spec = mel_spec * 255\n",
    "    mel_spec = np.round(mel_spec)    \n",
    "    mel_spec = mel_spec.astype('uint8')\n",
    "    mel_spec = np.asarray(mel_spec)\n",
    "    \n",
    "    bmp = Image.fromarray(mel_spec, 'L')\n",
    "    bmp.save('/courses/EDS232/rainforest/working/' + data[i][0] + '_' + data[i][1] + '_' + str(center) + '.bmp')\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Processed ' + str(i) + ' train examples from ' + str(len(data)))\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0067567c-7a89-40d2-b6c2-0fcf47728c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_birds = 24\n",
    "# 6GB GPU-friendly (~4 GB used by model)\n",
    "# Increase if neccesary\n",
    "batch_size = 16\n",
    "\n",
    "# This is enough to exactly reproduce results on local machine (Windows / Turing GPU)\n",
    "# Kaggle GPU kernels (Linux / Pascal GPU) are not deterministic even with random seeds set\n",
    "# Your score might vary a lot (~up to 0.05) on a different runs \n",
    "# due to picking different epochs to submit\n",
    "rng_seed = 1234\n",
    "random.seed(rng_seed)\n",
    "np.random.seed(rng_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(rng_seed)\n",
    "torch.manual_seed(rng_seed)\n",
    "torch.cuda.manual_seed(rng_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a552bb4c-ca2d-4c1c-8f7c-4cae15e1aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torchdata\n",
    "\n",
    "class RainforestDataset(torchdata.Dataset):\n",
    "    def __init__(self, filelist):\n",
    "        self.specs = []\n",
    "        self.labels = []\n",
    "        for f in filelist:\n",
    "            # Easier to pass species in filename at the start; \n",
    "            # worth changing later to more capable method\n",
    "            label = int(str.split(f, '_')[1])\n",
    "            label_array = np.zeros(num_birds, dtype=np.single)\n",
    "            label_array[label] = 1.\n",
    "            self.labels.append(label_array)\n",
    "            \n",
    "            # Open and save spectrogram to memory\n",
    "            \n",
    "            # If you use more spectrograms (add train_fp, for example), \n",
    "            # then they would not all fit to memory\n",
    "            # In this case you should load them on the fly in __getitem__\n",
    "            img = Image.open('/courses/EDS232/rainforest/working/' + f)\n",
    "            mel_spec = np.array(img)\n",
    "            img.close()\n",
    "            \n",
    "            # Transforming spectrogram from bmp to 0..1 array\n",
    "            mel_spec = mel_spec / 255\n",
    "            # Stacking for 3-channel image for resnet\n",
    "            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "            \n",
    "            self.specs.append(mel_spec)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.specs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # Augment here if you want\n",
    "        return self.specs[item], self.labels[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eb8d92e-f7bb-41f1-a66c-8419d59c4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 972 examples\n",
      "Validating on 244 examples\n"
     ]
    }
   ],
   "source": [
    "file_list = []\n",
    "label_list = []\n",
    "\n",
    "for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "    if '.bmp' in f:\n",
    "        file_list.append(f)\n",
    "        label = str.split(f, '_')[1]\n",
    "        label_list.append(label)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rng_seed)\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n",
    "    # Picking only first fold to train/val on\n",
    "    # This means loss of 20% training data\n",
    "    # To avoid this, you can train 5 different models on 5 folds and average predictions\n",
    "    if fold_id == 0:\n",
    "        train_files = np.take(file_list, train_index)\n",
    "        val_files = np.take(file_list, val_index)\n",
    "\n",
    "print('Training on ' + str(len(train_files)) + ' examples')\n",
    "print('Validating on ' + str(len(val_files)) + ' examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1879e146-da84-4271-8bbc-7651a9c76b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: cannot create /dev/null/: Is a directory\n"
     ]
    }
   ],
   "source": [
    "!pip install resnest > /dev/null/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4f01b5b-9f21-4dcc-8ec6-06af0d939aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 97.7M  100 97.7M    0     0   103M      0 --:--:-- --:--:-- --:--:--  103M\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from resnest.torch import resnest50\n",
    "import torch.nn.functional as F\n",
    "!cd /courses/EDS232/rainforest && curl -O https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d412c7ef-0f68-43a8-8eab-b747a6320091",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RainforestDataset(train_files)\n",
    "val_dataset = RainforestDataset(val_files)\n",
    "\n",
    "train_loader = torchdata.DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=torchdata.RandomSampler(train_dataset))\n",
    "\n",
    "val_loader = torchdata.DataLoader(val_dataset, \n",
    "                                  batch_size=batch_size, \n",
    "                                  sampler=torchdata.RandomSampler(val_dataset))\n",
    "\n",
    "# ResNeSt: Split-Attention Networks\n",
    "# https://arxiv.org/abs/2004.08955\n",
    "# Significantly outperforms standard Resnet\n",
    "\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_birds)\n",
    ")\n",
    "\n",
    "# Picked for this notebook; pick new ones after major changes \n",
    "# (such as adding train_fp to train data)\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.01, \n",
    "                            weight_decay=0.0001, \n",
    "                            momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                            step_size=7, \n",
    "                                            gamma=0.4)\n",
    "\n",
    "# This loss function is not exactly suited for competition metric, \n",
    "# which only cares about ranking of predictions\n",
    "# Exploring different loss fuctions would be a good idea\n",
    "pos_weights = torch.ones(num_birds)\n",
    "pos_weights = pos_weights * num_birds\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a391f1b8-3a7e-4ba3-9f0d-d2a117375bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop\n",
      "Epoch 0 training end. LR: 0.01, Loss: 1.3638783028868378, Correct answers: 65/972\n",
      "Epoch 0 validation end. LR: 0.01, Loss: 1.3503396734595299, Correct answers: 20/244\n",
      "Saving new best model at epoch 0 (20/244)\n",
      "Epoch 1 training end. LR: 0.01, Loss: 1.3188678651559549, Correct answers: 83/972\n",
      "Epoch 1 validation end. LR: 0.01, Loss: 1.3053582459688187, Correct answers: 25/244\n",
      "Saving new best model at epoch 1 (25/244)\n",
      "Epoch 2 training end. LR: 0.01, Loss: 1.21498306071172, Correct answers: 90/972\n",
      "Epoch 2 validation end. LR: 0.01, Loss: 1.2024907544255257, Correct answers: 20/244\n",
      "Epoch 3 training end. LR: 0.01, Loss: 1.1887264857526685, Correct answers: 89/972\n",
      "Epoch 3 validation end. LR: 0.01, Loss: 1.3292143195867538, Correct answers: 21/244\n",
      "Epoch 4 training end. LR: 0.01, Loss: 1.1440586819023382, Correct answers: 94/972\n",
      "Epoch 4 validation end. LR: 0.01, Loss: 1.2274692356586456, Correct answers: 20/244\n",
      "Epoch 5 training end. LR: 0.01, Loss: 1.125576355418221, Correct answers: 103/972\n",
      "Epoch 5 validation end. LR: 0.01, Loss: 1.162271998822689, Correct answers: 21/244\n",
      "Epoch 6 training end. LR: 0.01, Loss: 1.1129465845764661, Correct answers: 111/972\n",
      "Epoch 6 validation end. LR: 0.01, Loss: 1.076311182230711, Correct answers: 20/244\n",
      "Epoch 7 training end. LR: 0.004, Loss: 1.0512146246237832, Correct answers: 127/972\n",
      "Epoch 7 validation end. LR: 0.004, Loss: 1.0305317379534245, Correct answers: 31/244\n",
      "Saving new best model at epoch 7 (31/244)\n",
      "Epoch 8 training end. LR: 0.004, Loss: 1.023032706291949, Correct answers: 140/972\n",
      "Epoch 8 validation end. LR: 0.004, Loss: 1.0372957959771156, Correct answers: 38/244\n",
      "Saving new best model at epoch 8 (38/244)\n",
      "Epoch 9 training end. LR: 0.004, Loss: 1.0057943326527956, Correct answers: 142/972\n",
      "Epoch 9 validation end. LR: 0.004, Loss: 1.0342209339141846, Correct answers: 39/244\n",
      "Saving new best model at epoch 9 (39/244)\n",
      "Epoch 10 training end. LR: 0.004, Loss: 0.9961046938036309, Correct answers: 164/972\n",
      "Epoch 10 validation end. LR: 0.004, Loss: 1.0088021233677864, Correct answers: 40/244\n",
      "Saving new best model at epoch 10 (40/244)\n",
      "Epoch 11 training end. LR: 0.004, Loss: 0.9773712500196988, Correct answers: 171/972\n",
      "Epoch 11 validation end. LR: 0.004, Loss: 1.1004897020757198, Correct answers: 38/244\n",
      "Epoch 12 training end. LR: 0.004, Loss: 0.9674126535165505, Correct answers: 164/972\n",
      "Epoch 12 validation end. LR: 0.004, Loss: 1.0187463127076626, Correct answers: 39/244\n",
      "Epoch 13 training end. LR: 0.004, Loss: 0.9387336236531617, Correct answers: 193/972\n",
      "Epoch 13 validation end. LR: 0.004, Loss: 1.1174477711319923, Correct answers: 38/244\n",
      "Epoch 14 training end. LR: 0.0016, Loss: 0.9074887291329806, Correct answers: 204/972\n",
      "Epoch 14 validation end. LR: 0.0016, Loss: 0.9773037508130074, Correct answers: 46/244\n",
      "Saving new best model at epoch 14 (46/244)\n",
      "Epoch 15 training end. LR: 0.0016, Loss: 0.8589738353354032, Correct answers: 223/972\n",
      "Epoch 15 validation end. LR: 0.0016, Loss: 0.9946492277085781, Correct answers: 55/244\n",
      "Saving new best model at epoch 15 (55/244)\n",
      "Epoch 16 training end. LR: 0.0016, Loss: 0.8361774565743618, Correct answers: 241/972\n",
      "Epoch 16 validation end. LR: 0.0016, Loss: 0.9674517065286636, Correct answers: 52/244\n",
      "Epoch 17 training end. LR: 0.0016, Loss: 0.8257057901288642, Correct answers: 250/972\n",
      "Epoch 17 validation end. LR: 0.0016, Loss: 1.009031668305397, Correct answers: 43/244\n",
      "Epoch 18 training end. LR: 0.0016, Loss: 0.8610709661343059, Correct answers: 241/972\n",
      "Epoch 18 validation end. LR: 0.0016, Loss: 0.9639421738684177, Correct answers: 54/244\n",
      "Epoch 19 training end. LR: 0.0016, Loss: 0.8105488761526639, Correct answers: 276/972\n",
      "Epoch 19 validation end. LR: 0.0016, Loss: 1.0262306779623032, Correct answers: 53/244\n",
      "Epoch 20 training end. LR: 0.0016, Loss: 0.8086106357027273, Correct answers: 266/972\n",
      "Epoch 20 validation end. LR: 0.0016, Loss: 1.075862132012844, Correct answers: 39/244\n",
      "Epoch 21 training end. LR: 0.00064, Loss: 0.7377730422332639, Correct answers: 312/972\n",
      "Epoch 21 validation end. LR: 0.00064, Loss: 0.9859235547482967, Correct answers: 56/244\n",
      "Saving new best model at epoch 21 (56/244)\n",
      "Epoch 22 training end. LR: 0.00064, Loss: 0.7213050596049575, Correct answers: 301/972\n",
      "Epoch 22 validation end. LR: 0.00064, Loss: 0.9993421249091625, Correct answers: 54/244\n",
      "Epoch 23 training end. LR: 0.00064, Loss: 0.720540494215293, Correct answers: 324/972\n",
      "Epoch 23 validation end. LR: 0.00064, Loss: 1.061456598341465, Correct answers: 57/244\n",
      "Saving new best model at epoch 23 (57/244)\n",
      "Epoch 24 training end. LR: 0.00064, Loss: 0.7181924816037788, Correct answers: 305/972\n",
      "Epoch 24 validation end. LR: 0.00064, Loss: 1.0629728138446808, Correct answers: 56/244\n",
      "Epoch 25 training end. LR: 0.00064, Loss: 0.6996259425507217, Correct answers: 329/972\n",
      "Epoch 25 validation end. LR: 0.00064, Loss: 0.9977841787040234, Correct answers: 56/244\n",
      "Epoch 26 training end. LR: 0.00064, Loss: 0.6900419573314854, Correct answers: 345/972\n",
      "Epoch 26 validation end. LR: 0.00064, Loss: 1.10937887057662, Correct answers: 57/244\n",
      "Epoch 27 training end. LR: 0.00064, Loss: 0.6869794853398057, Correct answers: 321/972\n",
      "Epoch 27 validation end. LR: 0.00064, Loss: 1.118483480066061, Correct answers: 51/244\n",
      "Epoch 28 training end. LR: 0.00025600000000000004, Loss: 0.6613470882665916, Correct answers: 341/972\n",
      "Epoch 28 validation end. LR: 0.00025600000000000004, Loss: 1.0531313493847847, Correct answers: 68/244\n",
      "Saving new best model at epoch 28 (68/244)\n",
      "Epoch 29 training end. LR: 0.00025600000000000004, Loss: 0.6479872543303693, Correct answers: 362/972\n",
      "Epoch 29 validation end. LR: 0.00025600000000000004, Loss: 1.116099189966917, Correct answers: 56/244\n",
      "Epoch 30 training end. LR: 0.00025600000000000004, Loss: 0.6217601172259597, Correct answers: 378/972\n",
      "Epoch 30 validation end. LR: 0.00025600000000000004, Loss: 1.0671524070203304, Correct answers: 57/244\n",
      "Epoch 31 training end. LR: 0.00025600000000000004, Loss: 0.632248260935799, Correct answers: 377/972\n",
      "Epoch 31 validation end. LR: 0.00025600000000000004, Loss: 1.0208468399941921, Correct answers: 64/244\n",
      "Epoch 32 training end. LR: 0.00025600000000000004, Loss: 0.6200971955158672, Correct answers: 352/972\n",
      "Epoch 32 validation end. LR: 0.00025600000000000004, Loss: 1.0559903755784035, Correct answers: 61/244\n",
      "Epoch 33 training end. LR: 0.00025600000000000004, Loss: 0.613644038556052, Correct answers: 396/972\n",
      "Epoch 33 validation end. LR: 0.00025600000000000004, Loss: 1.0909987278282642, Correct answers: 59/244\n",
      "Epoch 34 training end. LR: 0.00025600000000000004, Loss: 0.5941707594472854, Correct answers: 388/972\n",
      "Epoch 34 validation end. LR: 0.00025600000000000004, Loss: 1.097803384065628, Correct answers: 57/244\n",
      "Epoch 35 training end. LR: 0.00010240000000000002, Loss: 0.602323162262557, Correct answers: 394/972\n",
      "Epoch 35 validation end. LR: 0.00010240000000000002, Loss: 1.103889063000679, Correct answers: 58/244\n",
      "Epoch 36 training end. LR: 0.00010240000000000002, Loss: 0.5981675156804381, Correct answers: 409/972\n",
      "Epoch 36 validation end. LR: 0.00010240000000000002, Loss: 1.0495924949645996, Correct answers: 61/244\n",
      "Epoch 37 training end. LR: 0.00010240000000000002, Loss: 0.6003047066633819, Correct answers: 386/972\n",
      "Epoch 37 validation end. LR: 0.00010240000000000002, Loss: 1.0994917415082455, Correct answers: 64/244\n",
      "Epoch 38 training end. LR: 0.00010240000000000002, Loss: 0.5760199339663397, Correct answers: 450/972\n",
      "Epoch 38 validation end. LR: 0.00010240000000000002, Loss: 1.0903446562588215, Correct answers: 63/244\n",
      "Epoch 39 training end. LR: 0.00010240000000000002, Loss: 0.5771904167581777, Correct answers: 417/972\n",
      "Epoch 39 validation end. LR: 0.00010240000000000002, Loss: 1.0929611772298813, Correct answers: 56/244\n"
     ]
    }
   ],
   "source": [
    "best_corrects = 0\n",
    "\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 40):\n",
    "    # Stats\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        vals, answers = torch.max(output, 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + \n",
    "          str(train_dataset.__len__()))\n",
    "    \n",
    "    # Single epoch - validation\n",
    "    with torch.no_grad():\n",
    "        # Stats\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        \n",
    "        model.eval()\n",
    "        for batch, (data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            # Stats\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "        \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, \n",
    "    # but in this case our loss is misaligned with competition metric, \n",
    "    # making accuracy a better metric\n",
    "    if sum(val_corr) > best_corrects:\n",
    "        print('Saving new best model at epoch ' + str(e) + ' (' + \n",
    "              str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "        torch.save(model, '/courses/EDS232/rainforest/best_model.pt')\n",
    "        best_corrects = sum(val_corr)\n",
    "        \n",
    "    # Call every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1086c821-9621-4f97-b52c-f6a67dacff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(f):\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/test/' + f, sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, \n",
    "                                                  n_fft=fft, \n",
    "                                                  hop_length=hop, \n",
    "                                                  sr=sr, \n",
    "                                                  fmin=fmin, \n",
    "                                                  fmax=fmax, \n",
    "                                                  power=1.5)\n",
    "        mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "          \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9258e2c9-8150-4fb5-8d06-17cebe07df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction loop\n",
      "1992\n",
      "Predicted for 100 of 1993 files\n",
      "Predicted for 200 of 1993 files\n",
      "Predicted for 300 of 1993 files\n",
      "Predicted for 400 of 1993 files\n",
      "Predicted for 500 of 1993 files\n",
      "Predicted for 600 of 1993 files\n",
      "Predicted for 700 of 1993 files\n",
      "Predicted for 800 of 1993 files\n",
      "Predicted for 900 of 1993 files\n",
      "Predicted for 1000 of 1993 files\n",
      "Predicted for 1100 of 1993 files\n",
      "Predicted for 1200 of 1993 files\n",
      "Predicted for 1300 of 1993 files\n",
      "Predicted for 1400 of 1993 files\n",
      "Predicted for 1500 of 1993 files\n",
      "Predicted for 1600 of 1993 files\n",
      "Predicted for 1700 of 1993 files\n",
      "Predicted for 1800 of 1993 files\n",
      "Predicted for 1900 of 1993 files\n",
      "Submission generated\n"
     ]
    }
   ],
   "source": [
    "save_to_disk = 0\n",
    "\n",
    "# Loading model back\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_birds)\n",
    ")\n",
    "\n",
    "model = torch.load('/courses/EDS232/rainforest/best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "# Scoring does not like many files:(\n",
    "if save_to_disk == 0:\n",
    "    for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "        os.remove('/courses/EDS232/rainforest/working/' + f)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10',\n",
    "                                's11', 's12','s13','s14','s15','s16','s17','s18','s19','s20','s21',\n",
    "                                's22','s23'])\n",
    "    \n",
    "    test_files = os.listdir('/courses/EDS232/rainforest/test/')\n",
    "    print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = load_test_file(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per bird species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, \n",
    "        # and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb6b84c-83af-4ae1-8200-23153377de8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
