{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a5663d8-7b91-49ae-9b15-ff2d05e9a1b8",
   "metadata": {},
   "source": [
    "# Rainforest Connection Species Audio Detection\n",
    "\n",
    "A Machine Learning project based on a [Kaggle Competition](https://www.kaggle.com/c/rfcx-species-audio-detection)\n",
    "\n",
    "Team Members: \n",
    "- Paloma Cartwright (palomacartwright@bren.ucsb.edu)\n",
    "- Halina Do-Linh (halina@bren.ucsb.edu)\n",
    "- Mia Forsline (mforsline@bren.ucsb.edu)\n",
    "- Dan Kerstan (danielkerstan@bren.ucsb.edu)\n",
    "- Scout Leonard (scout@bren.ucsb.edu)\n",
    "- Desik Somasundaram (desik@bren.ucsb.edu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5d155e-805b-4d69-b519-e93e80a96146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import csv\n",
    "import librosa\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from PIL import Image\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e00c72c4-6daa-47b5-9bfc-587ab3ab4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/courses/EDS232/rainforest\"\n",
    "fft = 2048\n",
    "hop = 512\n",
    "sr = 48000\n",
    "length = 10 * sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b276e9a-3e97-4e1a-b566-6cf264749103",
   "metadata": {},
   "source": [
    "## Read in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4751ac5-228d-490b-9429-ac9430298017",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/courses/EDS232/rainforest/train_tp.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    data = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444d77c-5bbd-4191-940a-82035f7223ac",
   "metadata": {},
   "source": [
    "### Set minimum and maximum frequency values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9cd9cd0-46d0-4b15-9f4b-04fe1bbb5196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum frequency: 84, maximum frequency: 15056\n"
     ]
    }
   ],
   "source": [
    "# Check minimum/maximum frequencies for species calls\n",
    "# Not neccesary, but there are usually plenty of noise in low frequencies, and removing it helps\n",
    "fmin = 24000\n",
    "fmax = 0\n",
    "\n",
    "# Skip header row (recording_id, species_id, songtype_id, t_min, f_min, t_max, f_max) \n",
    "# and start from 1 instead of 0\n",
    "for i in range(1, len(data)):\n",
    "    if fmin > float(data[i][4]):\n",
    "        fmin = float(data[i][4])\n",
    "    if fmax < float(data[i][6]):\n",
    "        fmax = float(data[i][6])\n",
    "\n",
    "# Get some safety margin\n",
    "fmin = int(fmin * 0.9)\n",
    "\n",
    "fmax = int(fmax * 1.1)\n",
    "print('Minimum frequency: ' + str(fmin) + ', maximum frequency: ' + str(fmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30827f9b-36d1-4075-b6df-8ad883f28a95",
   "metadata": {},
   "source": [
    "## Convert audio files to Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bfb12b0-9ebd-480c-adb4-4157434ae6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting spectrogram generation\n",
      "Processed 100 train examples from 1217\n",
      "Processed 200 train examples from 1217\n",
      "Processed 300 train examples from 1217\n",
      "Processed 400 train examples from 1217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1456102/1181287754.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting spectrogram generation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/courses/EDS232/rainforest/train/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.flac'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mt_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/librosa/util/decorators.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;31m# Load the target number of frames, and transpose to match librosa form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msf_desc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframe_duration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, frames, dtype, always_2d, fill_value, out)\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                 \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'read'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfill_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_array_io\u001b[0;34m(self, action, array, frames)\u001b[0m\n\u001b[1;32m   1310\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array_interface__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_io\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/soundfile.py\u001b[0m in \u001b[0;36m_cdata_io\u001b[0;34m(self, action, data, ctype, frames)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mcurr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_snd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sf_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'f_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mctype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0m_error_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errorcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Starting spectrogram generation')\n",
    "for i in range(1, len(data)):\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/train/' + data[i][0] + '.flac', sr=None)\n",
    "    \n",
    "    t_min = float(data[i][3]) * sr\n",
    "    t_max = float(data[i][5]) * sr\n",
    "    \n",
    "    # Positioning sound slice\n",
    "    center = np.round((t_min + t_max) / 2)\n",
    "    beginning = center - length / 2\n",
    "    if beginning < 0:\n",
    "        beginning = 0\n",
    "    \n",
    "    ending = beginning + length\n",
    "    if ending > len(wav):\n",
    "        ending = len(wav)\n",
    "        beginning = ending - length\n",
    "        \n",
    "    slice = wav[int(beginning):int(ending)]\n",
    "    \n",
    "    # Mel spectrogram generation\n",
    "    # Default settings were bad, parameters are adjusted to generate somewhat \n",
    "    # reasonable quality images\n",
    "    # The better your images are, the better your neural net would perform\n",
    "    # You can also use librosa.stft + librosa.amplitude_to_db instead\n",
    "    mel_spec = librosa.feature.melspectrogram(slice, \n",
    "                                              n_fft = fft, \n",
    "                                              hop_length = hop, \n",
    "                                              sr = sr, \n",
    "                                              fmin = fmin, \n",
    "                                              fmax = fmax, \n",
    "                                              power = 1.5)\n",
    "    mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "    # Normalize to 0...1 - this is what goes into neural net\n",
    "    mel_spec = mel_spec - np.min(mel_spec)\n",
    "    mel_spec = mel_spec / np.max(mel_spec)\n",
    "\n",
    "    # And this 0...255 is for the saving in bmp format\n",
    "    mel_spec = mel_spec * 255\n",
    "    mel_spec = np.round(mel_spec)    \n",
    "    mel_spec = mel_spec.astype('uint8')\n",
    "    mel_spec = np.asarray(mel_spec)\n",
    "    \n",
    "    bmp = Image.fromarray(mel_spec, 'L')\n",
    "    bmp.save('/courses/EDS232/rainforest/working/' + data[i][0] + '_' + \n",
    "             data[i][1] + '_' + str(center) + '.bmp')\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('Processed ' + str(i) + ' train examples from ' + str(len(data)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067567c-7a89-40d2-b6c2-0fcf47728c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_species = 24\n",
    "# 6GB GPU-friendly (~4 GB used by model)\n",
    "# Increase if neccesary\n",
    "batch_size = 16\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d7c9a-3d1a-4233-bbe1-28f691af63d3",
   "metadata": {},
   "source": [
    "## Creating the RainforestDataset Class for the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552bb4c-ca2d-4c1c-8f7c-4cae15e1aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as torchdata\n",
    "\n",
    "class RainforestDataset(torchdata.Dataset):\n",
    "    def __init__(self, filelist):\n",
    "        self.specs = []\n",
    "        self.labels = []\n",
    "        for f in filelist:\n",
    "            # Easier to pass species in filename at the start; \n",
    "            # worth changing later to more capable method\n",
    "            label = int(str.split(f, '_')[1])\n",
    "            label_array = np.zeros(num_species, dtype=np.single)\n",
    "            label_array[label] = 1.\n",
    "            self.labels.append(label_array)\n",
    "            \n",
    "            # Open and save spectrogram to memory\n",
    "            \n",
    "            # If you use more spectrograms (add train_fp, for example), \n",
    "            # then they would not all fit to memory\n",
    "            # In this case you should load them on the fly in __getitem__\n",
    "            img = Image.open('/courses/EDS232/rainforest/working/' + f)\n",
    "            mel_spec = np.array(img)\n",
    "            img.close()\n",
    "            \n",
    "            # Transforming spectrogram from bmp to 0..1 array\n",
    "            mel_spec = mel_spec / 255\n",
    "            # Stacking for 3-channel image for resnet\n",
    "            mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "            \n",
    "            self.specs.append(mel_spec)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.specs)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # Augment here if you want\n",
    "        return self.specs[item], self.labels[item]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda832b5-bf63-49dd-9932-849235b72d89",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb8d92e-f7bb-41f1-a66c-8419d59c4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "label_list = []\n",
    "\n",
    "for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "    if '.bmp' in f:\n",
    "        file_list.append(f)\n",
    "        label = str.split(f, '_')[1]\n",
    "        label_list.append(label)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "train_files = []\n",
    "val_files = []\n",
    "\n",
    "for fold_id, (train_index, val_index) in enumerate(skf.split(file_list, label_list)):\n",
    "    # Picking only first fold to train/val on\n",
    "    # This means loss of 20% training data\n",
    "    # To avoid this, you can train 5 different models on 5 folds and average predictions\n",
    "    if fold_id == 0:\n",
    "        train_files = np.take(file_list, train_index)\n",
    "        val_files = np.take(file_list, val_index)\n",
    "\n",
    "print('Training on ' + str(len(train_files)) + ' examples')\n",
    "print('Validating on ' + str(len(val_files)) + ' examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aac64c-1f5d-400d-b891-1677a32eeffa",
   "metadata": {},
   "source": [
    "## Preparing for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d412c7ef-0f68-43a8-8eab-b747a6320091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from resnest.torch import resnest50\n",
    "import torch.nn.functional as F\n",
    "# !cd /courses/EDS232/rainforest && curl -O https://download.pytorch.org/models/resnet50-19c8e357.pth\n",
    "\n",
    "train_dataset = RainforestDataset(train_files)\n",
    "val_dataset = RainforestDataset(val_files)\n",
    "\n",
    "train_loader = torchdata.DataLoader(train_dataset, \n",
    "                                    batch_size=batch_size, \n",
    "                                    sampler=torchdata.RandomSampler(train_dataset))\n",
    "\n",
    "val_loader = torchdata.DataLoader(val_dataset, \n",
    "                                  batch_size=batch_size, \n",
    "                                  sampler=torchdata.RandomSampler(val_dataset))\n",
    "\n",
    "# ResNeSt: Split-Attention Networks\n",
    "# https://arxiv.org/abs/2004.08955\n",
    "# Significantly outperforms standard Resnet\n",
    "\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "# Picked for this notebook; pick new ones after major changes \n",
    "# (such as adding train_fp to train data)\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            lr=0.01, \n",
    "                            weight_decay=0.0001, \n",
    "                            momentum=0.9)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                            step_size=7, \n",
    "                                            gamma=0.4)\n",
    "\n",
    "# This loss function is not exactly suited for competition metric, \n",
    "# which only cares about ranking of predictions\n",
    "# Exploring different loss fuctions would be a good idea\n",
    "pos_weights = torch.ones(num_species)\n",
    "pos_weights = pos_weights * num_species\n",
    "loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    loss_function = loss_function.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f912c9b-9b02-4db3-b6eb-aba415a180c5",
   "metadata": {},
   "source": [
    "## Training model on Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a391f1b8-3a7e-4ba3-9f0d-d2a117375bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_corrects = 0\n",
    "\n",
    "# Train loop\n",
    "print('Starting training loop')\n",
    "for e in range(0, 40):\n",
    "    # Stats\n",
    "    train_loss = []\n",
    "    train_corr = []\n",
    "    \n",
    "    # Single epoch - train\n",
    "    model.train()\n",
    "    for batch, (data, target) in enumerate(train_loader):\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Stats\n",
    "        vals, answers = torch.max(output, 1)\n",
    "        vals, targets = torch.max(target, 1)\n",
    "        corrects = 0\n",
    "        for i in range(0, len(answers)):\n",
    "            if answers[i] == targets[i]:\n",
    "                corrects = corrects + 1\n",
    "        train_corr.append(corrects)\n",
    "        \n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    for g in optimizer.param_groups:\n",
    "        lr = g['lr']\n",
    "    print('Epoch ' + str(e) + ' training end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(train_loss) / len(train_loss)) +\n",
    "          ', Correct answers: ' + str(sum(train_corr)) + '/' + \n",
    "          str(train_dataset.__len__()))\n",
    "    \n",
    "    # Single epoch - validation\n",
    "    with torch.no_grad():\n",
    "        # Stats\n",
    "        val_loss = []\n",
    "        val_corr = []\n",
    "        \n",
    "        model.eval()\n",
    "        for batch, (data, target) in enumerate(val_loader):\n",
    "            data = data.float()\n",
    "            if torch.cuda.is_available():\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            \n",
    "            # Stats\n",
    "            vals, answers = torch.max(output, 1)\n",
    "            vals, targets = torch.max(target, 1)\n",
    "            corrects = 0\n",
    "            for i in range(0, len(answers)):\n",
    "                if answers[i] == targets[i]:\n",
    "                    corrects = corrects + 1\n",
    "            val_corr.append(corrects)\n",
    "        \n",
    "            val_loss.append(loss.item())\n",
    "    \n",
    "    # Stats\n",
    "    print('Epoch ' + str(e) + ' validation end. LR: ' + str(lr) + \n",
    "          ', Loss: ' + str(sum(val_loss) / len(val_loss)) +\n",
    "          ', Correct answers: ' + str(sum(val_corr)) + '/' + str(val_dataset.__len__()))\n",
    "    \n",
    "    # If this epoch is better than previous on validation, save model\n",
    "    # Validation loss is the more common metric, \n",
    "    # but in this case our loss is misaligned with competition metric, \n",
    "    # making accuracy a better metric\n",
    "    if sum(val_corr) > best_corrects:\n",
    "        print('Saving new best model at epoch ' + str(e) + ' (' + \n",
    "              str(sum(val_corr)) + '/' + str(val_dataset.__len__()) + ')')\n",
    "        torch.save(model, '/courses/EDS232/rainforest/best_model.pt')\n",
    "        best_corrects = sum(val_corr)\n",
    "        \n",
    "    # Call every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "# Free memory\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be892a30-47e0-4aee-b8af-5067a40a39df",
   "metadata": {},
   "source": [
    "### Function to split and load one test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086c821-9621-4f97-b52c-f6a67dacff8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_file(f):\n",
    "    wav, sr = librosa.load('/courses/EDS232/rainforest/test/' + f, sr=None)\n",
    "\n",
    "    # Split for enough segments to not miss anything\n",
    "    segments = len(wav) / length\n",
    "    segments = int(np.ceil(segments))\n",
    "    \n",
    "    mel_array = []\n",
    "    \n",
    "    for i in range(0, segments):\n",
    "        # Last segment going from the end\n",
    "        if (i + 1) * length > len(wav):\n",
    "            slice = wav[len(wav) - length:len(wav)]\n",
    "        else:\n",
    "            slice = wav[i * length:(i + 1) * length]\n",
    "        \n",
    "        # Same mel spectrogram as before\n",
    "        mel_spec = librosa.feature.melspectrogram(slice, \n",
    "                                                  n_fft=fft, \n",
    "                                                  hop_length=hop, \n",
    "                                                  sr=sr, \n",
    "                                                  fmin=fmin, \n",
    "                                                  fmax=fmax, \n",
    "                                                  power=1.5)\n",
    "        mel_spec = resize(mel_spec, (224, 400))\n",
    "    \n",
    "        mel_spec = mel_spec - np.min(mel_spec)\n",
    "        mel_spec = mel_spec / np.max(mel_spec)\n",
    "          \n",
    "        mel_spec = np.stack((mel_spec, mel_spec, mel_spec))\n",
    "\n",
    "        mel_array.append(mel_spec)\n",
    "    \n",
    "    return mel_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f46ca4-9f52-4ccf-89ec-ff72287989e6",
   "metadata": {},
   "source": [
    "## Predict Species for each recording using Best Model\n",
    "\n",
    "The best model was stored in `best_model.pt` output file from the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258e2c9-8150-4fb5-8d06-17cebe07df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_disk = 0\n",
    "\n",
    "# Loading model back\n",
    "model = resnest50(pretrained=False)\n",
    "\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(2048, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(p=0.2),\n",
    "    nn.Linear(1024, num_species)\n",
    ")\n",
    "\n",
    "model = torch.load('/courses/EDS232/rainforest/best_model.pt')\n",
    "model.eval()\n",
    "\n",
    "# Scoring does not like many files:(\n",
    "if save_to_disk == 0:\n",
    "    for f in os.listdir('/courses/EDS232/rainforest/working/'):\n",
    "        os.remove('/courses/EDS232/rainforest/working/' + f)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "# Prediction loop\n",
    "print('Starting prediction loop')\n",
    "with open('/courses/EDS232/rainforest/submission.csv', 'w', newline='') as csvfile:\n",
    "    submission_writer = csv.writer(csvfile, delimiter=',')\n",
    "    submission_writer.writerow(['recording_id','s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10',\n",
    "                                's11', 's12','s13','s14','s15','s16','s17','s18','s19','s20','s21',\n",
    "                                's22','s23'])\n",
    "    \n",
    "    test_files = os.listdir('/courses/EDS232/rainforest/test/')\n",
    "    print(len(test_files))\n",
    "    \n",
    "    # Every test file is split on several chunks and prediction is made for each chunk\n",
    "    for i in range(0, len(test_files)):\n",
    "        data = load_test_file(test_files[i])\n",
    "        data = torch.tensor(data)\n",
    "        data = data.float()\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        # Taking max prediction from all slices per species\n",
    "        # Usually you want Sigmoid layer here to convert output to probabilities\n",
    "        # In this competition only relative ranking matters, \n",
    "        # and not the exact value of prediction, so we can use it directly\n",
    "        maxed_output = torch.max(output, dim=0)[0]\n",
    "        maxed_output = maxed_output.cpu().detach()\n",
    "        \n",
    "        file_id = str.split(test_files[i], '.')[0]\n",
    "        write_array = [file_id]\n",
    "        \n",
    "        for out in maxed_output:\n",
    "            write_array.append(out.item())\n",
    "    \n",
    "        submission_writer.writerow(write_array)\n",
    "        \n",
    "        if i % 100 == 0 and i > 0:\n",
    "            print('Predicted for ' + str(i) + ' of ' + str(len(test_files) + 1) + ' files')\n",
    "\n",
    "print('Submission generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9d775-a469-4665-8e21-b63ff68bbd55",
   "metadata": {},
   "source": [
    "## Perform Logit Transformation on Submission Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b6f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(os.path.join(base_dir, \"submission.csv\"))\n",
    "\n",
    "#make the recording id the index\n",
    "submission = submission.set_index(\"recording_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18e1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new csv file with sigmoid values\n",
    "\n",
    "with open(\"/courses/EDS232/rainforest/submission_logit.csv\", 'w', newline = '') as csvfile2:\n",
    "    \n",
    "    logit_sub = submission.applymap(sigmoid)\n",
    "    logit_sub.to_csv(csvfile2, header = ['s0','s1','s2','s3','s4','s5','s6','s7','s8','s9','s10',\n",
    "                                's11', 's12','s13','s14','s15','s16','s17','s18','s19','s20','s21',\n",
    "                                's22','s23'])\n",
    "\n",
    "print(\"submission generated, gorls!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
